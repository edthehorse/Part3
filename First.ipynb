{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoOgWX/0D2hotWTUyxMoK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edthehorse/Part3/blob/master/First.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmQtMQs0rhtb",
        "outputId": "77972891-0f50-431a-d00d-0e5a423df575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9Olz4XWrl92",
        "outputId": "e4dc53e0-6953-42ff-e8b9-5e77f22a1c42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "NUdjK7Lsr211"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "OB__5UOVr-nH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "DTJNMA1bsDds"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwt7k41_sJS0",
        "outputId": "0b18a604-284b-480c-e44a-ac41b6b0c3e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = 'NLP is a very powerful tool'"
      ],
      "metadata": {
        "id": "wpoUQFmqsMuU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_doc = nlp(raw_text)"
      ],
      "metadata": {
        "id": "UZq7I23MsT80"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "list_stopwords=list(stopwords)"
      ],
      "metadata": {
        "id": "9Ex831D2sXbM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in list_stopwords[:100]:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng-ADMn5up1M",
        "outputId": "259daa62-6f48-482b-c774-065dab1bb4b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yet\n",
            "about\n",
            "name\n",
            "ours\n",
            "whereby\n",
            "least\n",
            "it\n",
            "than\n",
            "another\n",
            "thereupon\n",
            "still\n",
            "each\n",
            "latterly\n",
            "nobody\n",
            "much\n",
            "around\n",
            "very\n",
            "whenever\n",
            "none\n",
            "their\n",
            "onto\n",
            "what\n",
            "who\n",
            "but\n",
            "really\n",
            "seem\n",
            "top\n",
            "twenty\n",
            "‘ve\n",
            "hereby\n",
            "last\n",
            "up\n",
            "get\n",
            "anyway\n",
            "herein\n",
            "she\n",
            "done\n",
            "per\n",
            "towards\n",
            "twelve\n",
            "if\n",
            "you\n",
            "without\n",
            "'re\n",
            "myself\n",
            "whatever\n",
            "enough\n",
            "us\n",
            "in\n",
            "two\n",
            "throughout\n",
            "wherein\n",
            "ourselves\n",
            "yourselves\n",
            "under\n",
            "which\n",
            "down\n",
            "due\n",
            "hundred\n",
            "then\n",
            "across\n",
            "full\n",
            "did\n",
            "however\n",
            "all\n",
            "’m\n",
            "various\n",
            "thence\n",
            "‘s\n",
            "between\n",
            "most\n",
            "were\n",
            "ten\n",
            "was\n",
            "someone\n",
            "via\n",
            "an\n",
            "'d\n",
            "mine\n",
            "some\n",
            "that\n",
            "have\n",
            "hereupon\n",
            "from\n",
            "own\n",
            "already\n",
            "to\n",
            "the\n",
            "everything\n",
            "anywhere\n",
            "nowhere\n",
            "formerly\n",
            "no\n",
            "sometime\n",
            "third\n",
            "been\n",
            "few\n",
            "forty\n",
            "never\n",
            "noone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text= 'Amazon Alexa, also known simply as Alexa, is a virtual assistant AI technology developed by Amazon, first used in the Amazon Echo smart speakers developed by Amazon Lab126. It is capable of voice interaction, music playback, making to-do lists, setting alarms, streaming podcasts, playing audiobooks, and providing weather, traffic, sports, and other real-time information, such as news. Alexa can also control several smart devices using itself as a home automation system. Users are able to extend the Alexa capabilities by installing \"skills\" additional functionality developed by third-party vendors, in other settings more commonly called apps such as weather programs and audio features.Most devices with Alexa allow users to activate the device using a wake-word (such as Alexa or Amazon); other devices (such as the Amazon mobile app on iOS or Android and Amazon Dash Wand) require the user to push a button to activate Alexa listening mode, although, some phones also allow a user to say a command, such as \"Alexa\" or \"Alexa wake\". Currently, interaction and communication with Alexa are available only in English, German, French, Italian, Spanish, Portuguese, Japanese, and Hindi. In Canada, Alexa is available in English and French with the Quebec accent).[6][7]As of November 2018, Amazon had more than 10,000 employees working on Alexa and related products.[8] In January 2019, Amazons devices team announced that they had sold over 100 million Alexa-enabled devices.[9]In September 2019, Amazon launched many new devices achieving many records while competing with the worlds smart home industry. The new Echo Studio became the first smart speaker with 360 sound and Dolby sound. Other new devices included an Echo dot with a clock behind the fabric, a new third-generation Amazon Echo, Echo Show 8, a plug-in Echo device, Echo Flex, Alexa built-in wireless earphones, Echo buds, Alexa built-in spectacles, Echo frames, an Alexa built-in Ring, and Echo Loop.Alexa can perform a number of preset functions out-of-the-box such as set timers, share the current weather, create lists, access Wikipedia articles, and many more things. Users say a designated \"wake word\" (the default is simply \"Alexa\") to alert an Alexa-enabled device of an ensuing function command. Alexa listens for the command and performs the appropriate function, or skill, to answer a question or command. Alexas question answering ability is partly powered by the Wolfram Language. When questions are asked, Alexa converts sound waves into text which allows it to gather information from various sources. Behind the scenes, the data gathered is then parsed by Wolfram technology to generate suitable and accurate answers. Alexa-supported devices can stream music from the owners Amazon Music accounts and have built-in support for Pandora and Spotify accounts. Alexa can play music from streaming services such as Apple Music and Google Play Music from a phone or tablet.In addition to performing pre-set functions, Alexa can also perform additional functions through third-party skills that users can enable. Some of the most popular Alexa skills in 2018 included \"Question of the Day\" and \"National Geographic Geo Quiz\" for trivia; \"TuneIn Live\" to listen to live sporting events and news stations; \"Big Sky\" for hyper local weather updates; \"Sleep and Relaxation Sounds\" for listening to calming sounds; \"Sesame Street\" for children entertainment; and \"Fitbit\" for Fitbit users who want to check in on their health stats. In 2019, Apple, Google, Amazon, and Zigbee Alliance announced a partnership to make smart home products work together.'\n",
        "text_doc=nlp(raw_text)\n",
        "\n",
        "token_count_without_stopwords=0\n",
        "\n",
        "# Filtring out the stopwords\n",
        "filtered_text= [token for token in text_doc if not token.is_stop]\n",
        "\n",
        "# Counting the tokens after removal of stopwords\n",
        "for token in filtered_text:\n",
        "    print(token)\n",
        "    token_count_without_stopwords+=1"
      ],
      "metadata": {
        "id": "mVk6TBOkuv2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "gqY-NRQHxAf_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "words = ['dance ','dances', 'dancing ','danced']\n",
        "\n",
        "for word in words:\n",
        "    print(word.ljust(8),'-------', ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_wiL6CKxB4E",
        "outputId": "1dba8a18-4f54-491d-e2ee-1c2d541986d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dance    ------- dance \n",
            "dances   ------- danc\n",
            "dancing  ------- dancing \n",
            "danced   ------- danc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=' I had calculated it wrong.I am calculating the bill again.'\n",
        "text_doc=nlp(text)\n",
        "\n",
        "filter_text=[token for token in text_doc if not token.is_stop]\n",
        "print(filter_text)\n",
        "filter_text=[token for token in filter_text if not token.is_punct]\n",
        "print(filter_text)\n",
        "\n",
        "\" \".join([token.lemma_ for token in filter_text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Gvp0h14Hzep8",
        "outputId": "6a7e55cd-a99f-4fd3-aa5e-58e27c8ce2cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ , calculated, wrong, ., calculating, bill, .]\n",
            "[ , calculated, wrong, calculating, bill]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  calculate wrong calculate bill'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dpHSzV9nxHiU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}